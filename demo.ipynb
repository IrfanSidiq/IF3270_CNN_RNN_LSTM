{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.tensor import Tensor\n",
    "from src.activation_function import Linear, ReLU, Sigmoid, Softmax\n",
    "from src.loss_function import MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[3. 8.], grad=[0. 0.], op='*')\n",
      "Tensor(data=[0.33333333 0.5       ], grad=[0. 0.], op='*')\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='Linear')\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='ReLU')\n",
      "Tensor(data=[4.], grad=[0.], op='MeanSquaredError')\n",
      "Tensor(data=[4.], grad=[0.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[ -8. -12.], grad=[0. 0.], op='*')\n",
      "Tensor(data=[ -8. -12.], grad=[0. 0.], op='Linear')\n",
      "Tensor(data=[0. 0.], grad=[0. 0.], op='ReLU')\n",
      "Tensor(data=[125.], grad=[0.], op='MeanSquaredError')\n",
      "Tensor(data=[1.], grad=[0.], op='MeanSquaredError')\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Tensor(data=[1. 2.], grad=[ -54. -156.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[198. 390.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[36. 52.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[ -72. -156.], op='+')\n",
      "Tensor(data=[ -8. -12.], grad=[ -9. -13.], op='*')\n",
      "Tensor(data=[ -8. -12.], grad=[ -9. -13.], op='Linear')\n",
      "Tensor(data=[0. 0.], grad=[-1. -1.], op='ReLU')\n",
      "Tensor(data=[125.], grad=[1.], op='MeanSquaredError')\n",
      "Tensor(data=[1.], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Tensor(data=[2. 3. 4.], grad=[0. 0. 0.], op='None', type=weight)\n",
      "Tensor(data=[3. 4. 5.], grad=[0. 0. 0.], op='None', type=weight)\n",
      "Tensor(data=[ 2.  6. 12.], grad=[0. 0. 0.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[0. 0. 0.], op='*')\n",
      "Tensor(data=[20.], grad=[0.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[0.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[0.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[0.], op='ReLU')\n",
      "Tensor(data=[20. 26.], grad=[0. 0.], op='concatenate(axis=0)')\n",
      "Tensor(data=[80.], grad=[0.], op='MeanSquaredError')\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Tensor(data=[2. 3. 4.], grad=[ 4.  8. 12.], op='None', type=weight)\n",
      "Tensor(data=[3. 4. 5.], grad=[12. 24. 36.], op='None', type=weight)\n",
      "Tensor(data=[ 2.  6. 12.], grad=[4. 4. 4.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[12. 12. 12.], op='*')\n",
      "Tensor(data=[20.], grad=[4.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[12.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[4.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[12.], op='ReLU')\n",
      "Tensor(data=[20. 26.], grad=[ 4. 12.], op='concatenate(axis=0)')\n",
      "Tensor(data=[80.], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "y = np.array([16, 14])                                          # correct class / y_true\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Initiate automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(4,)\n",
      "============ Layer 1 ============\n",
      "Tensor(data=[ 2.  6. 12.], grad=[5831915. 5831915. 5831915.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[6932435. 6932435. 6932435.], op='*')\n",
      "Tensor(data=[ 4. 10. 18.], grad=[8032955. 8032955. 8032955.], op='*')\n",
      "Tensor(data=[20.], grad=[5831915.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[6932435.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[32.], grad=[8032955.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[5831915.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[6932435.], op='ReLU')\n",
      "Tensor(data=[32.], grad=[8032955.], op='ReLU')\n",
      "Tensor(data=[20. 26. 32.], grad=[5831915. 6932435. 8032955.], op='concatenate(axis=0)')\n",
      "\n",
      "============ Layer 2 ============\n",
      "Tensor(data=[ 1. 20. 26. 32.], grad=[4731395. 5831915. 6932435. 8032955.], op='add_x0')\n",
      "Tensor(data=[  2.  60. 104. 160.], grad=[154241. 154241. 154241. 154241.], op='*')\n",
      "Tensor(data=[  3.  80. 130. 192.], grad=[187172.5 187172.5 187172.5 187172.5], op='*')\n",
      "Tensor(data=[  4. 100. 156. 224.], grad=[220104. 220104. 220104. 220104.], op='*')\n",
      "Tensor(data=[  5. 120. 182. 256.], grad=[253035.5 253035.5 253035.5 253035.5], op='*')\n",
      "Tensor(data=[  6. 140. 208. 288.], grad=[285967. 285967. 285967. 285967.], op='*')\n",
      "Tensor(data=[326.], grad=[154241.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[405.], grad=[187172.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[484.], grad=[220104.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[563.], grad=[253035.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[642.], grad=[285967.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[326.], grad=[154241.], op='ReLU')\n",
      "Tensor(data=[405.], grad=[187172.5], op='ReLU')\n",
      "Tensor(data=[484.], grad=[220104.], op='ReLU')\n",
      "Tensor(data=[563.], grad=[253035.5], op='ReLU')\n",
      "Tensor(data=[642.], grad=[285967.], op='ReLU')\n",
      "Tensor(data=[326. 405. 484. 563. 642.], grad=[154241.  187172.5 220104.  253035.5 285967. ], op='concatenate(axis=0)')\n",
      "\n",
      "============ Layer 3 ============\n",
      "Tensor(data=[  1. 326. 405. 484. 563. 642.], grad=[121309.5 154241.  187172.5 220104.  253035.5 285967. ], op='add_x0')\n",
      "Tensor(data=[2.000e+00 9.780e+02 1.620e+03 2.420e+03 3.378e+03 4.494e+03], grad=[6421. 6421. 6421. 6421. 6421. 6421.], op='*')\n",
      "Tensor(data=[3.000e+00 1.304e+03 2.025e+03 2.904e+03 3.941e+03 5.136e+03], grad=[7624.5 7624.5 7624.5 7624.5 7624.5 7624.5], op='*')\n",
      "Tensor(data=[4.000e+00 1.630e+03 2.430e+03 3.388e+03 4.504e+03 5.778e+03], grad=[8836. 8836. 8836. 8836. 8836. 8836.], op='*')\n",
      "Tensor(data=[5.000e+00 1.956e+03 2.835e+03 3.872e+03 5.067e+03 6.420e+03], grad=[10050. 10050. 10050. 10050. 10050. 10050.], op='*')\n",
      "Tensor(data=[12892.], grad=[6421.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[15313.], grad=[7624.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[17734.], grad=[8836.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20155.], grad=[10050.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[12892.], grad=[6421.], op='ReLU')\n",
      "Tensor(data=[15313.], grad=[7624.5], op='ReLU')\n",
      "Tensor(data=[17734.], grad=[8836.], op='ReLU')\n",
      "Tensor(data=[20155.], grad=[10050.], op='ReLU')\n",
      "Tensor(data=[12892. 15313. 17734. 20155.], grad=[ 6421.   7624.5  8836.  10050. ], op='concatenate(axis=0)')\n",
      "Tensor(data=[2.78439637e+08], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "## Simulation of 3-layered (excluding input layer) network with n = [3, 5, 4] number of neurons\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            \n",
    "y = np.array([50, 64, 62, 55])                                          \n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")\n",
    "wh3 = Tensor(np.array([4, 5, 6]), tensor_type=\"weight\")\n",
    "wh4 = Tensor(np.array([2, 3, 4, 5]), tensor_type=\"weight\")\n",
    "wh5 = Tensor(np.array([3, 4, 5, 6]), tensor_type=\"weight\")\n",
    "wh6 = Tensor(np.array([4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh7 = Tensor(np.array([5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh8 = Tensor(np.array([6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh9 = Tensor(np.array([2, 3, 4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh10 = Tensor(np.array([3, 4, 5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh11 = Tensor(np.array([4, 5, 6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh12 = Tensor(np.array([5, 6, 7, 8, 9, 10]), tensor_type=\"weight\")\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Layer 3\n",
    "input_l3 = output_l2.add_x0()\n",
    "wh9_l3 = wh9 * input_l3\n",
    "wh10_l3 = wh10 * input_l3\n",
    "wh11_l3 = wh11 * input_l3\n",
    "wh12_l3 = wh12 * input_l3\n",
    "net9 = wh9_l3.sum()\n",
    "net10 = wh10_l3.sum()\n",
    "net11 = wh11_l3.sum()\n",
    "net12 = wh12_l3.sum()\n",
    "o9 = net9.compute_activation(ReLU)\n",
    "o10 = net10.compute_activation(ReLU)\n",
    "o11 = net11.compute_activation(ReLU)\n",
    "o12 = net12.compute_activation(ReLU)\n",
    "output_l3 = o9.concat([o10, o11, o12])\n",
    "\n",
    "# Compute loss\n",
    "loss = output_l3.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"============ Layer 1 ============\")\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(wh3_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(net3)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(output_l1)\n",
    "print(\"\\n============ Layer 2 ============\")\n",
    "print(input_l2)\n",
    "print(wh4_l2)\n",
    "print(wh5_l2)\n",
    "print(wh6_l2)\n",
    "print(wh7_l2)\n",
    "print(wh8_l2)\n",
    "print(net4)\n",
    "print(net5)\n",
    "print(net6)\n",
    "print(net7)\n",
    "print(net8)\n",
    "print(o4)\n",
    "print(o5)\n",
    "print(o6)\n",
    "print(o7)\n",
    "print(o8)\n",
    "print(output_l2)\n",
    "print(\"\\n============ Layer 3 ============\")\n",
    "print(input_l3)\n",
    "print(wh9_l3)\n",
    "print(wh10_l3)\n",
    "print(wh11_l3)\n",
    "print(wh12_l3)\n",
    "print(net9)\n",
    "print(net10)\n",
    "print(net11)\n",
    "print(net12)\n",
    "print(o9)\n",
    "print(o10)\n",
    "print(o11)\n",
    "print(o12)\n",
    "print(output_l3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation for input with 3 features and batch_size = 3\n",
    "x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), tensor_type=\"input\")\n",
    "y = np.array([\n",
    "    [50, 60, 70, 80, 90],\n",
    "    [55, 65, 75, 85, 95],\n",
    "    [52, 62, 72, 82, 92]\n",
    "])\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Compute loss\n",
    "loss = output_l2.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer Test Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Test Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN Test Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM Test Case**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
