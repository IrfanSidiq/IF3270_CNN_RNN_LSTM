{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\tf-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical as keras_to_categorical\n",
    "\n",
    "from src.core import Tensor\n",
    "from src.functions import Linear, ReLU, Softmax, MeanSquaredError, CategoricalCrossEntropy\n",
    "from src.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten\n",
    "from src.model import Sequential\n",
    "from src.optimization import Optimizer, Adam\n",
    "from src.tests import TestCNNLayer, TestCNNModel, TestLoadKerasWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[3. 8.], grad=[0. 0.], op='*')\n",
      "Tensor(data=[0.33333333 0.5       ], grad=[0. 0.], op='*')\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='Linear')\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='ReLU')\n",
      "Tensor(data=[4.], grad=[0.], op='MeanSquaredError')\n",
      "Tensor(data=[4.], grad=[0.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Tensor(data=[1. 2.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[0. 0.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[0. 0.], op='+')\n",
      "Tensor(data=[ -8. -12.], grad=[0. 0.], op='*')\n",
      "Tensor(data=[ -8. -12.], grad=[0. 0.], op='Linear')\n",
      "Tensor(data=[0. 0.], grad=[0. 0.], op='ReLU')\n",
      "Tensor(data=[125.], grad=[0.], op='MeanSquaredError')\n",
      "Tensor(data=[1.], grad=[0.], op='MeanSquaredError')\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Tensor(data=[1. 2.], grad=[ -54. -156.], op='None')\n",
      "Tensor(data=[3. 4.], grad=[198. 390.], op='None')\n",
      "Tensor(data=[4. 6.], grad=[36. 52.], op='+')\n",
      "Tensor(data=[-2. -2.], grad=[ -72. -156.], op='+')\n",
      "Tensor(data=[ -8. -12.], grad=[ -9. -13.], op='*')\n",
      "Tensor(data=[ -8. -12.], grad=[ -9. -13.], op='Linear')\n",
      "Tensor(data=[0. 0.], grad=[-1. -1.], op='ReLU')\n",
      "Tensor(data=[125.], grad=[1.], op='MeanSquaredError')\n",
      "Tensor(data=[1.], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Tensor(data=[2. 3. 4.], grad=[0. 0. 0.], op='None', type=weight)\n",
      "Tensor(data=[3. 4. 5.], grad=[0. 0. 0.], op='None', type=weight)\n",
      "Tensor(data=[ 2.  6. 12.], grad=[0. 0. 0.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[0. 0. 0.], op='*')\n",
      "Tensor(data=[20.], grad=[0.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[0.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[0.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[0.], op='ReLU')\n",
      "Tensor(data=[20. 26.], grad=[0. 0.], op='concatenate(axis=0)')\n",
      "Tensor(data=[80.], grad=[0.], op='MeanSquaredError')\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Tensor(data=[2. 3. 4.], grad=[ 4.  8. 12.], op='None', type=weight)\n",
      "Tensor(data=[3. 4. 5.], grad=[12. 24. 36.], op='None', type=weight)\n",
      "Tensor(data=[ 2.  6. 12.], grad=[4. 4. 4.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[12. 12. 12.], op='*')\n",
      "Tensor(data=[20.], grad=[4.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[12.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[4.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[12.], op='ReLU')\n",
      "Tensor(data=[20. 26.], grad=[ 4. 12.], op='concatenate(axis=0)')\n",
      "Tensor(data=[80.], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "y = np.array([16, 14])                                          # correct class / y_true\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Initiate automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Layer 1 ============\n",
      "Tensor(data=[ 2.  6. 12.], grad=[5831915. 5831915. 5831915.], op='*')\n",
      "Tensor(data=[ 3.  8. 15.], grad=[6932435. 6932435. 6932435.], op='*')\n",
      "Tensor(data=[ 4. 10. 18.], grad=[8032955. 8032955. 8032955.], op='*')\n",
      "Tensor(data=[20.], grad=[5831915.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[26.], grad=[6932435.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[32.], grad=[8032955.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20.], grad=[5831915.], op='ReLU')\n",
      "Tensor(data=[26.], grad=[6932435.], op='ReLU')\n",
      "Tensor(data=[32.], grad=[8032955.], op='ReLU')\n",
      "Tensor(data=[20. 26. 32.], grad=[5831915. 6932435. 8032955.], op='concatenate(axis=0)')\n",
      "\n",
      "============ Layer 2 ============\n",
      "Tensor(data=[ 1. 20. 26. 32.], grad=[4731395. 5831915. 6932435. 8032955.], op='add_x0')\n",
      "Tensor(data=[  2.  60. 104. 160.], grad=[154241. 154241. 154241. 154241.], op='*')\n",
      "Tensor(data=[  3.  80. 130. 192.], grad=[187172.5 187172.5 187172.5 187172.5], op='*')\n",
      "Tensor(data=[  4. 100. 156. 224.], grad=[220104. 220104. 220104. 220104.], op='*')\n",
      "Tensor(data=[  5. 120. 182. 256.], grad=[253035.5 253035.5 253035.5 253035.5], op='*')\n",
      "Tensor(data=[  6. 140. 208. 288.], grad=[285967. 285967. 285967. 285967.], op='*')\n",
      "Tensor(data=[326.], grad=[154241.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[405.], grad=[187172.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[484.], grad=[220104.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[563.], grad=[253035.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[642.], grad=[285967.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[326.], grad=[154241.], op='ReLU')\n",
      "Tensor(data=[405.], grad=[187172.5], op='ReLU')\n",
      "Tensor(data=[484.], grad=[220104.], op='ReLU')\n",
      "Tensor(data=[563.], grad=[253035.5], op='ReLU')\n",
      "Tensor(data=[642.], grad=[285967.], op='ReLU')\n",
      "Tensor(data=[326. 405. 484. 563. 642.], grad=[154241.  187172.5 220104.  253035.5 285967. ], op='concatenate(axis=0)')\n",
      "\n",
      "============ Layer 3 ============\n",
      "Tensor(data=[  1. 326. 405. 484. 563. 642.], grad=[121309.5 154241.  187172.5 220104.  253035.5 285967. ], op='add_x0')\n",
      "Tensor(data=[2.000e+00 9.780e+02 1.620e+03 2.420e+03 3.378e+03 4.494e+03], grad=[6421. 6421. 6421. 6421. 6421. 6421.], op='*')\n",
      "Tensor(data=[3.000e+00 1.304e+03 2.025e+03 2.904e+03 3.941e+03 5.136e+03], grad=[7624.5 7624.5 7624.5 7624.5 7624.5 7624.5], op='*')\n",
      "Tensor(data=[4.000e+00 1.630e+03 2.430e+03 3.388e+03 4.504e+03 5.778e+03], grad=[8836. 8836. 8836. 8836. 8836. 8836.], op='*')\n",
      "Tensor(data=[5.000e+00 1.956e+03 2.835e+03 3.872e+03 5.067e+03 6.420e+03], grad=[10050. 10050. 10050. 10050. 10050. 10050.], op='*')\n",
      "Tensor(data=[12892.], grad=[6421.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[15313.], grad=[7624.5], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[17734.], grad=[8836.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[20155.], grad=[10050.], op='sum(axis=None, keepdims=False)')\n",
      "Tensor(data=[12892.], grad=[6421.], op='ReLU')\n",
      "Tensor(data=[15313.], grad=[7624.5], op='ReLU')\n",
      "Tensor(data=[17734.], grad=[8836.], op='ReLU')\n",
      "Tensor(data=[20155.], grad=[10050.], op='ReLU')\n",
      "Tensor(data=[12892. 15313. 17734. 20155.], grad=[ 6421.   7624.5  8836.  10050. ], op='concatenate(axis=0)')\n",
      "Tensor(data=[2.78439637e+08], grad=[1.], op='MeanSquaredError')\n"
     ]
    }
   ],
   "source": [
    "## Simulation of 3-layered (excluding input layer) network with n = [3, 5, 4] number of neurons\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            \n",
    "y = np.array([50, 64, 62, 55])                                          \n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")\n",
    "wh3 = Tensor(np.array([4, 5, 6]), tensor_type=\"weight\")\n",
    "wh4 = Tensor(np.array([2, 3, 4, 5]), tensor_type=\"weight\")\n",
    "wh5 = Tensor(np.array([3, 4, 5, 6]), tensor_type=\"weight\")\n",
    "wh6 = Tensor(np.array([4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh7 = Tensor(np.array([5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh8 = Tensor(np.array([6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh9 = Tensor(np.array([2, 3, 4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh10 = Tensor(np.array([3, 4, 5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh11 = Tensor(np.array([4, 5, 6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh12 = Tensor(np.array([5, 6, 7, 8, 9, 10]), tensor_type=\"weight\")\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Layer 3\n",
    "input_l3 = output_l2.add_x0()\n",
    "wh9_l3 = wh9 * input_l3\n",
    "wh10_l3 = wh10 * input_l3\n",
    "wh11_l3 = wh11 * input_l3\n",
    "wh12_l3 = wh12 * input_l3\n",
    "net9 = wh9_l3.sum()\n",
    "net10 = wh10_l3.sum()\n",
    "net11 = wh11_l3.sum()\n",
    "net12 = wh12_l3.sum()\n",
    "o9 = net9.compute_activation(ReLU)\n",
    "o10 = net10.compute_activation(ReLU)\n",
    "o11 = net11.compute_activation(ReLU)\n",
    "o12 = net12.compute_activation(ReLU)\n",
    "output_l3 = o9.concat([o10, o11, o12])\n",
    "\n",
    "# Compute loss\n",
    "loss = output_l3.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"============ Layer 1 ============\")\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(wh3_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(net3)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(output_l1)\n",
    "print(\"\\n============ Layer 2 ============\")\n",
    "print(input_l2)\n",
    "print(wh4_l2)\n",
    "print(wh5_l2)\n",
    "print(wh6_l2)\n",
    "print(wh7_l2)\n",
    "print(wh8_l2)\n",
    "print(net4)\n",
    "print(net5)\n",
    "print(net6)\n",
    "print(net7)\n",
    "print(net8)\n",
    "print(o4)\n",
    "print(o5)\n",
    "print(o6)\n",
    "print(o7)\n",
    "print(o8)\n",
    "print(output_l2)\n",
    "print(\"\\n============ Layer 3 ============\")\n",
    "print(input_l3)\n",
    "print(wh9_l3)\n",
    "print(wh10_l3)\n",
    "print(wh11_l3)\n",
    "print(wh12_l3)\n",
    "print(net9)\n",
    "print(net10)\n",
    "print(net11)\n",
    "print(net12)\n",
    "print(o9)\n",
    "print(o10)\n",
    "print(o11)\n",
    "print(o12)\n",
    "print(output_l3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation for input with 3 features and batch_size = 3\n",
    "x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), tensor_type=\"input\")\n",
    "y = np.array([\n",
    "    [50, 60, 70, 80, 90],\n",
    "    [55, 65, 75, 85, 95],\n",
    "    [52, 62, 72, 82, 92]\n",
    "])\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Compute loss\n",
    "loss = output_l2.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Grad Check: AveragePooling2DLayer ---\n",
      "AveragePooling gradient check PASSED.\n",
      "\n",
      "--- Grad Check: FlattenLayer ---\n",
      "Flatten gradient check PASSED.\n",
      "\n",
      "--- Forward Test Large: N=2,C_in=3,H=32,W=32,K=8,KS=(3, 3),S=(2, 2),P='valid',Bias=False,Act=None ---\n",
      "Forward Test Large completed in 0.0075 seconds.\n",
      "\n",
      "--- Forward Test Large: N=4,C_in=8,H=28,W=28,K=16,KS=(5, 5),S=(1, 1),P='same',Bias=True,Act=ReLU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Test Large completed in 0.0615 seconds.\n",
      "\n",
      "--- Grad Check Large: N=1,C_in=3,H=10,W=10,K=2,KS=(3, 3),S=(1, 1),P='valid',Bias=False,Act=None ---\n",
      "Calculating numerical kernel gradient...\n",
      "Numerical kernel gradient calculated.\n",
      "Calculating analytical kernel gradient...\n",
      "Analytical kernel gradient calculated.\n",
      "Kernel gradient check PASSED (Large Input).\n",
      "Grad Check Large completed in 0.0073 seconds.\n",
      "\n",
      "--- Grad Check Large: N=2,C_in=2,H=8,W=8,K=3,KS=(3, 3),S=(2, 2),P='same',Bias=True,Act=ReLU ---\n",
      "Calculating numerical kernel gradient...\n",
      "Numerical kernel gradient calculated.\n",
      "Calculating analytical kernel gradient...\n",
      "Analytical kernel gradient calculated.\n",
      "Kernel gradient check PASSED (Large Input).\n",
      "Calculating numerical bias gradient...\n",
      "Numerical bias gradient calculated.\n",
      "Bias gradient check PASSED (Large Input).\n",
      "Grad Check Large completed in 0.0210 seconds.\n",
      "\n",
      "--- Grad Check: MaxPooling2DLayer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPooling gradient check PASSED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 0.205s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Grad Check: MaxPooling2DLayer (Overlapping) ---\n",
      "MaxPooling overlapping gradient check PASSED.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=14 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit tests for Convolution layer\n",
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestCNNLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TestSequentialModel: Compile with specific instances/classes ---\n",
      "Model compiled with optimizer Adam and loss MeanSquaredError.\n",
      "Compile with Adam instance and MSE class: PASSED\n",
      "\n",
      "--- Integration Test: Deeper CNN End-to-End Gradient Check ---\n",
      "Calculating numerical gradients for Conv1 Kernel...\n",
      "Numerical gradients for Conv1 Kernel calculated.\n",
      "Calculating numerical gradients for Conv1 Bias...\n",
      "Numerical gradients for Conv1 Bias calculated.\n",
      "Calculating numerical gradients for Conv2 Kernel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.162s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical gradients for Conv2 Kernel calculated.\n",
      "Calculating numerical gradients for Conv2 Bias...\n",
      "Numerical gradients for Conv2 Bias calculated.\n",
      "Calculating analytical gradients...\n",
      "Analytical gradients calculated.\n",
      "Asserting gradients for Conv1 Kernel...\n",
      "Asserting gradients for Conv1 Bias...\n",
      "Asserting gradients for Conv2 Kernel...\n",
      "Asserting gradients for Conv2 Bias...\n",
      "Deeper CNN Integration Test completed in 0.0734 seconds.\n",
      "\n",
      "--- TestSequentialModel: Full Fit, Predict, Evaluate Cycle ---\n",
      "Model compiled with optimizer Adam and loss MeanSquaredError.\n",
      "\n",
      "Starting model.fit()...\n",
      "Epoch 1/5\n",
      "  loss: 0.3377 - val_loss: 0.3532 - 0.01s/epoch                   \n",
      "Epoch 2/5\n",
      "  loss: 0.3375 - val_loss: 0.3532 - 0.01s/epoch                   \n",
      "Epoch 3/5\n",
      "  loss: 0.3375 - val_loss: 0.3532 - 0.01s/epoch                   \n",
      "Epoch 4/5\n",
      "  loss: 0.3375 - val_loss: 0.3532 - 0.01s/epoch                   \n",
      "Epoch 5/5\n",
      "  loss: 0.3375 - val_loss: 0.3532 - 0.01s/epoch                   \n",
      "Initial loss: 0.33766290014828576, Final loss: 0.3374853517831923\n",
      "\n",
      "Starting model.predict()...\n",
      "\n",
      "Starting model.evaluate() on validation data...\n",
      "Evaluating...\n",
      "  2/2 [██████████████████████████████] 100.0%\n",
      "Evaluation - loss: 0.3532\n",
      "\n",
      "Starting model.evaluate() on training data...\n",
      "\n",
      "--- TestSequentialModel: Full Cycle Test PASSED ---\n",
      "\n",
      "--- TestSequentialModel: Get Parameters ---\n",
      "Get Parameters test PASSED.\n",
      "\n",
      "--- Integration Test: Small CNN End-to-End Gradient Check ---\n",
      "Calculating numerical gradients for Conv1 Kernel...\n",
      "Numerical gradients for Conv1 Kernel calculated.\n",
      "Calculating numerical gradients for Conv1 Bias...\n",
      "Numerical gradients for Conv1 Bias calculated.\n",
      "Calculating analytical gradients...\n",
      "Analytical gradients calculated.\n",
      "Conv1 Kernel gradient check PASSED.\n",
      "Conv1 Bias gradient check PASSED.\n",
      "Integration Test completed in 0.0125 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit test for a full CNN model architecture\n",
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestCNNModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TestKerasLoad: Multi Conv/Pool ---\n",
      "WARNING:tensorflow:From c:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\tf-env\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\tf-env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmp8ogz3w1k\\test_keras_weights.h5\n",
      "Warning: Layer group 'c_pool1' (for custom layer 'c_pool1') not found in H5 file. Available groups: ['k_conv1', 'k_conv2', 'k_flatten', 'k_pool1', 'k_pool2', 'top_level_model_weights'] Skipping.\n",
      "Warning: Layer group 'c_pool2' (for custom layer 'c_pool2') not found in H5 file. Available groups: ['k_conv1', 'k_conv2', 'k_flatten', 'k_pool1', 'k_pool2', 'top_level_model_weights'] Skipping.\n",
      "Warning: Layer group 'c_flatten' (for custom layer 'c_flatten') not found in H5 file. Available groups: ['k_conv1', 'k_conv2', 'k_flatten', 'k_pool1', 'k_pool2', 'top_level_model_weights'] Skipping.\n",
      "Successfully loaded weights into 5 matched layers.\n",
      "Multi Conv/Pool weight loading test PASSED.\n",
      "\n",
      "--- TestKerasLoad: Simple CNN ---\n",
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmp3n7udbck\\test_keras_weights.h5\n",
      "Warning: Layer group 'my_pool1' (for custom layer 'my_pool1') not found in H5 file. Available groups: ['conv2d_keras_1', 'flatten_keras_1', 'maxpool_keras_1', 'top_level_model_weights'] Skipping.\n",
      "Warning: Layer group 'my_flatten' (for custom layer 'my_flatten') not found in H5 file. Available groups: ['conv2d_keras_1', 'flatten_keras_1', 'maxpool_keras_1', 'top_level_model_weights'] Skipping.\n",
      "Successfully loaded weights into 3 matched layers.\n",
      "Simple CNN weight loading test PASSED.\n",
      "\n",
      "--- TestKerasLoad: Skip Missing / Mismatched Names ---\n",
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmpzcumq_pl\\test_keras_weights.h5\n",
      "Warning: Layer group 'my_flatten' (for custom layer 'my_flatten') not found in H5 file. Available groups: ['conv1_k', 'conv2_k_extra', 'flatten_k', 'top_level_model_weights'] Skipping.\n",
      "Successfully loaded weights into 2 matched layers.\n",
      "Skip missing (H5 has extra layer, map used) PASSED.\n",
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmpzcumq_pl\\test_keras_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.405s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on mapped layer not in H5 (skip=False) PASSED.\n",
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmpzcumq_pl\\test_keras_weights.h5\n",
      "Error on direct name match not in H5 (skip=False) PASSED.\n",
      "Loading weights from Keras H5 file: C:\\Users\\IRFANS~1\\AppData\\Local\\Temp\\tmpzcumq_pl\\test_keras_weights.h5\n",
      "Warning: Layer group 'my_conv_not_in_keras' (for custom layer 'my_conv_not_in_keras') not found in H5 file. Available groups: ['conv1_k', 'conv2_k_extra', 'flatten_k', 'top_level_model_weights'] Skipping.\n",
      "Successfully loaded weights into 2 matched layers.\n",
      "Skip missing (Custom model has extra layer, skip=True) PASSED.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestLoadKerasWeights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CIFAR-10 Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x_train shape: (50000, 32, 32, 3), y_train_raw shape: (50000, 1)\n",
      "Transposed x_train shape: (50000, 3, 32, 32)\n",
      "One-hot y_train shape: (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train_raw), (x_test, y_test_raw) = cifar10.load_data()\n",
    "\n",
    "print(f\"Original x_train shape: {x_train.shape}, y_train_raw shape: {y_train_raw.shape}\")\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = np.transpose(x_train, (0, 3, 1, 2))\n",
    "x_test = np.transpose(x_test, (0, 3, 1, 2))\n",
    "print(f\"Transposed x_train shape: {x_train.shape}\")\n",
    "\n",
    "num_classes = 10\n",
    "y_train = keras_to_categorical(y_train_raw, num_classes)\n",
    "y_test = keras_to_categorical(y_test_raw, num_classes)\n",
    "print(f\"One-hot y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = x_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_kernels=32, kernel_size=3, input_channels=input_channels, padding='same', activation=ReLU, name=\"conv1\"),\n",
    "    Conv2D(num_kernels=32, kernel_size=3, padding='same', activation=ReLU, name=\"conv2\"),\n",
    "    MaxPooling2D(pool_size=2, strides=2, name=\"pool1\"),\n",
    "\n",
    "    Conv2D(num_kernels=64, kernel_size=3, padding='same', activation=ReLU, name=\"conv3\"),\n",
    "    Conv2D(num_kernels=64, kernel_size=3, padding='same', activation=ReLU, name=\"conv4\"),\n",
    "    MaxPooling2D(pool_size=2, strides=2, name=\"pool2\"),\n",
    "\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(units=512, activation_class=ReLU, name=\"dense1\"),\n",
    "    Dense(units=num_classes, activation_class=Softmax, name=\"output_dense\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with optimizer Adam and loss CategoricalCrossEntropy.\n",
      "\n",
      "Starting training for 10 epochs with batch size 64...\n",
      "Using 45000 samples for training, 5000 for validation during fit.\n",
      "Epoch 1/10\n",
      "  3/704 [------------------------------] 0.4%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m     x_train_fit, y_train_fit \u001b[38;5;241m=\u001b[39m x_train, y_train\n\u001b[0;32m     26\u001b[0m     validation_for_fit \u001b[38;5;241m=\u001b[39m (x_test, y_test) \u001b[38;5;28;01mif\u001b[39;00m x_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_for_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\src\\model\\sequential.py:165\u001b[0m, in \u001b[0;36mSequential.fit\u001b[1;34m(self, x_train, y_train, epochs, batch_size, validation_data, shuffle, verbose)\u001b[0m\n\u001b[0;32m    162\u001b[0m x_batch_np \u001b[38;5;241m=\u001b[39m x_train_shuffled[start_idx:end_idx]\n\u001b[0;32m    163\u001b[0m y_batch_np \u001b[38;5;241m=\u001b[39m y_train_shuffled[start_idx:end_idx]\n\u001b[1;32m--> 165\u001b[0m batch_loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch_np\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m epoch_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss_value \u001b[38;5;241m*\u001b[39m (end_idx \u001b[38;5;241m-\u001b[39m start_idx) \u001b[38;5;66;03m# Weighted by actual batch size\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m num_batches \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# After last batch, clear progress bar line for epoch summary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\src\\model\\sequential.py:98\u001b[0m, in \u001b[0;36mSequential._training_step\u001b[1;34m(self, x_batch_tensor, y_batch_np)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs a single training step: forward, loss, backward, optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 98\u001b[0m y_pred_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# self.latest_output_tensor is set\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Calculate loss using the Tensor's compute_loss method\u001b[39;00m\n\u001b[0;32m    101\u001b[0m current_loss_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_output_tensor\u001b[38;5;241m.\u001b[39mcompute_loss(y_batch_np, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn_class)\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\src\\model\\sequential.py:52\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input_tensor, training)\u001b[0m\n\u001b[0;32m     50\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_output_tensor \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\src\\layers\\pooling.py:75\u001b[0m, in \u001b[0;36mMaxPooling2D.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     72\u001b[0m receptive_field \u001b[38;5;241m=\u001b[39m x_padded_data[n, c, h_start:h_end, w_start:w_end]\n\u001b[0;32m     73\u001b[0m output_data[n, c, h, w] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(receptive_field)\n\u001b[1;32m---> 75\u001b[0m idx_in_field \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munravel_index(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreceptive_field\u001b[49m\u001b[43m)\u001b[49m, receptive_field\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_indices[n, c, h, w, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m h_start \u001b[38;5;241m+\u001b[39m idx_in_field[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_indices[n, c, h, w, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m w_start \u001b[38;5;241m+\u001b[39m idx_in_field[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\tf-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_CNN_RNN_LSTM\\tf-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_function_class = CategoricalCrossEntropy \n",
    "model.compile(optimizer=optimizer, loss=loss_function_class)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs with batch size {BATCH_SIZE}...\")\n",
    "\n",
    "num_train_samples = x_train.shape[0]\n",
    "if num_train_samples > 1000:\n",
    "    val_split_idx = int(num_train_samples * 0.9)\n",
    "    x_train_fit, y_train_fit = x_train[:val_split_idx], y_train[:val_split_idx]\n",
    "    x_val_fit, y_val_fit = x_train[val_split_idx:], y_train[val_split_idx:]\n",
    "    validation_for_fit = (x_val_fit, y_val_fit)\n",
    "    print(f\"Using {x_train_fit.shape[0]} samples for training, {x_val_fit.shape[0]} for validation during fit.\")\n",
    "else:\n",
    "    print(\"Using test set for validation during fit (not best practice for final eval).\")\n",
    "    x_train_fit, y_train_fit = x_train, y_train\n",
    "    validation_for_fit = (x_test, y_test) if x_test is not None else None\n",
    "\n",
    "history = model.fit(x_train_fit, y_train_fit, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=validation_for_fit,\n",
    "    shuffle=True, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history and 'loss' in history:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history and not all(np.isnan(history['val_loss'])):\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history to plot.\")\n",
    "\n",
    "if x_test is not None and y_test is not None:\n",
    "    print(\"\\nEvaluating on the test set:\")\n",
    "    test_loss = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "    \n",
    "    predictions_test_np = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "    predicted_classes_test = np.argmax(predictions_test_np, axis=1)\n",
    "    true_classes_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy_test = np.mean(predicted_classes_test == true_classes_test)\n",
    "    print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"No test data available for final evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
